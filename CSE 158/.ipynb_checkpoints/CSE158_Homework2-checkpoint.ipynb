{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.io.arff import loadarff \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/5year.arff'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c007d4aa74f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadarff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/5year.arff\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0myear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/scipy/io/arff/arffread.py\u001b[0m in \u001b[0;36mloadarff\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mofile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0mofile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_loadarff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mofile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/5year.arff'"
     ]
    }
   ],
   "source": [
    "raw_data = loadarff(\"./data/5year.arff\")\n",
    "year = pd.DataFrame(raw_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year['class'] = year['class'].apply(lambda x: 0 if x == \n",
    "                                    year['class'][0] else 1)\n",
    "year = year.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 = LogisticRegression(C = 1.0)\n",
    "X_train = year.drop(columns='class')\n",
    "y_train = year['class']\n",
    "question_1.fit(X_train,y_train)\n",
    "y_predict = question_1.predict(X_train)\n",
    "print('Accuracy:', accuracy_score(y_train, y_predict))\n",
    "print(\"Balanced Error Rate (BER):\", \n",
    "      1-balanced_accuracy_score(y_train, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "* Accuracy: 0.9663477400197954\n",
    "* Balanced Error Rate (BER): 0.4810749837661251"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_2 = LogisticRegression(class_weight=\"balanced\",C=1.0)\n",
    "X_train = year.drop(columns='class')\n",
    "y_train = year['class']\n",
    "question_2.fit(X_train,y_train)\n",
    "y_predict = question_2.predict(X_train)\n",
    "print('Accuracy:', accuracy_score(y_train, y_predict))\n",
    "print(\"Balanced Error Rate (BER):\", \n",
    "      1-balanced_accuracy_score(y_train, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "* Accuracy: 0.7825800065984824\n",
    "* Balanced Error Rate (BER): 0.20712081350122835"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    year.drop(columns='class'), year['class'], test_size=0.5)\n",
    "X_test, X_vali, y_test, y_vali = train_test_split(\n",
    "                                X_test, y_test, test_size=0.5)\n",
    "question_3 = LogisticRegression(class_weight=\"balanced\")\n",
    "question_3.fit(X_train,y_train)\n",
    "\n",
    "print('Training data set: ')\n",
    "y_predict_on_train = question_3.predict(X_train)\n",
    "print('Accuracy:', accuracy_score(y_train, y_predict_on_train))\n",
    "print(\"Balanced Error Rate (BER):\", 1-balanced_accuracy_score(y_train,\n",
    "                                                    y_predict_on_train))\n",
    "print('-----------------------------------------------------------')\n",
    "\n",
    "print('Test data set: ')\n",
    "y_predict_on_test = question_3.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_predict_on_test))\n",
    "print(\"Balanced Error Rate (BER):\", 1-balanced_accuracy_score(y_test, \n",
    "                                                y_predict_on_test))\n",
    "print('-----------------------------------------------------------')\n",
    "\n",
    "print('Validation data set: ')\n",
    "y_predict_on_vali = question_3.predict(X_vali)\n",
    "print('Accuracy:', accuracy_score(y_vali, y_predict_on_vali))\n",
    "print(\"Balanced Error Rate (BER):\", 1-balanced_accuracy_score(y_vali, \n",
    "                                                y_predict_on_vali))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "* Training data set: \n",
    "    * Accuracy: 0.7676567656765677\n",
    "    * Balanced Error Rate (BER): 0.2640062089379829\n",
    "\n",
    "* Test data set: \n",
    "    * Accuracy: 0.7559366754617414\n",
    "    * Balanced Error Rate (BER): 0.31184321143337534\n",
    "* Validation data set: \n",
    "    * Accuracy: 0.7651715039577837\n",
    "    * Balanced Error Rate (BER): 0.2999074140018517"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    year.drop(columns='class'), year['class'], test_size=0.5)\n",
    "X_test, X_vali, y_test, y_vali = train_test_split(\n",
    "    X_test, y_test, test_size=0.5)\n",
    "report = []\n",
    "for i in range(-4,5):\n",
    "    question_4 = LogisticRegression(class_weight=\"balanced\",C=10**i)\n",
    "    question_4.fit(X_train,y_train)\n",
    "    y_predict_on_train = question_4.predict(X_train)\n",
    "    y_predict_on_test = question_4.predict(X_test)\n",
    "    y_predict_on_vali = question_4.predict(X_vali)\n",
    "    BEC_train = 1 - balanced_accuracy_score(y_train, y_predict_on_train)\n",
    "    BEC_test = 1 - balanced_accuracy_score(y_test, y_predict_on_test)\n",
    "    BEC_vali = 1 - balanced_accuracy_score(y_vali, y_predict_on_vali)\n",
    "    report.append([BEC_train,BEC_test,BEC_vali])\n",
    "report = pd.DataFrame(report,index=np.arange(-4,5)).T\n",
    "report.index=['train','test','vali']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "* I would choose C = 10^2 = 100 as the C. BER means that the average of the proportion of wrong classifications in each class. So, the lower the BER score, the better the model is. In the above table, when C = 100, the validation data set has the lowest balanced error rate. Therefore, in training and testing data sets, they have lowest BER rate among all other C options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_score(beta,y_true,y_predict):\n",
    "    # calculate TP, TN, FP, FN\n",
    "    def matrix(true_val,pred_val):\n",
    "        true_val = np.array(true_val)\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        for i in range(len(true_val)):\n",
    "            if true_val[i]== pred_val[i]==1:\n",
    "                TP += 1\n",
    "            if true_val[i]==pred_val[i]==0:\n",
    "                TN += 1\n",
    "            if true_val[i] == 1 and pred_val[i]!= true_val[i]:\n",
    "                FN += 1\n",
    "            if true_val[i] == 0 and pred_val[i]!=true_val[i]:\n",
    "                FP += 1\n",
    "        return (TP, TN, FP, FN)\n",
    "    TP, TN, FP, FN = matrix(y_true,y_predict)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    return ((1 + beta**2) * (precision * recall)) / ((beta**2) * precision + recall)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "* Fβ scores for β = 0.1 : 0.6362204724409449\n",
    "* Fβ scores for β = 1 : 0.19512195121951217\n",
    "* Fβ scores for β = 10 : 0.1152310325156874"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    year.drop(columns='class'),year['class'], test_size=0.5)\n",
    "X_test, X_vali, y_test, y_vali = train_test_split(\n",
    "    X_test, y_test, test_size=0.5)\n",
    "\n",
    "question_7 = PCA(n_components = X_train.shape[1])\n",
    "question_7.fit(X_train)\n",
    "print(question_7.components_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = []\n",
    "for i in range(5,31,5):\n",
    "    Xpca_train = np.matmul(np.array(X_train),\n",
    "                           question_7.components_[:,:i])\n",
    "    Xpca_valid = np.matmul(np.array(X_vali),\n",
    "                           question_7.components_[:,:i])\n",
    "    Xpca_test = np.matmul(np.array(X_test),\n",
    "                          question_7.components_[:,:i])\n",
    "    logistic_q8 = LogisticRegression(class_weight=\"balanced\",C=1)\n",
    "    logistic_q8.fit(Xpca_train,y_train)\n",
    "    y_predict_on_test = logistic_q8.predict(Xpca_test)\n",
    "    y_predict_on_vali = logistic_q8.predict(Xpca_valid)\n",
    "    BEC_test = 1 - balanced_accuracy_score(y_test, y_predict_on_test)\n",
    "    BEC_vali = 1 - balanced_accuracy_score(y_vali, y_predict_on_vali)\n",
    "    report.append([BEC_test,BEC_vali])\n",
    "report = pd.DataFrame(report,index=np.arange(5,31,5)).T\n",
    "report.index=['test','vali']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
